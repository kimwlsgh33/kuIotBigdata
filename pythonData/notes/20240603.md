# 로지스틱 함수

역전파 -> 미분값 -> 레이어의 개수 -> 전파가 잘 안됨 -> 적분값이 너무 큼

# 지도 학습

타겟이 있는

According to the result,

## Regression (회귀)

**수치형 데이터**

#### 평가항목 [RSS]

- RMSE
- RSE
- MAE
- MAPE

#### 선형회귀

- 다중선형회귀
- 가중회귀

#### 다항회귀

- 스폰라인

## Classification (분류)

**범주형 데이터**

Naive Bayes
판별 분석

#### 평가항목 [RSS]

- 최대우도
- 혼동행렬 (confusion matrix)
- 정밀도, 재현물, 특이도
- ROC, AUC

# 비지도 학습

타겟이 없는

# Machine Learning

train -> validation
chap6 시작 - 통계적 머신러닝

통계적 머신러닝, 앙상블 학습, 의사결정 트리는 모두 데이터 분석 및 예측을 위한 머신러닝 분야에서 중요한 개념들입니다. 각각에 대해 자세히 설명하겠습니다.

1. 통계적 머신러닝 (Statistical Machine Learning)

통계적 머신러닝은 통계학의 원리를 활용하여 데이터로부터 예측 모델을 구축하는 방법입니다. 이 접근법은 데이터의 확률 분포를 추정하거나 통계 모델을 통해 데이터의 패턴을 학습합니다. 주요 개념은 다음과 같습니다:

    * 회귀 분석 (Regression Analysis): 연속적인 값을 예측하기 위해 사용됩니다. 예를 들어, 주택 가격 예측, 주식 가격 예측 등이 있습니다.
    * 분류 (Classification): 데이터가 어떤 카테고리에 속하는지 예측합니다. 예를 들어, 이메일이 스팸인지 아닌지 분류하는 것 등이 있습니다.
    * 클러스터링 (Clustering): 데이터 포인트를 비슷한 특성을 가진 그룹으로 나눕니다. 예를 들어, 고객 세분화 등이 있습니다.

2. 앙상블 학습 (Ensemble Learning)

앙상블 학습은 여러 개의 모델을 결합하여 더 나은 성능을 얻는 방법입니다. 개별 모델의 예측을 종합하여 최종 예측을 도출하기 때문에 일반적으로 단일 모델보다 더 정확하고 견고한 결과를 제공합니다. 대표적인 앙상블 기법으로는 다음이 있습니다:

    * 배깅 (Bagging): 여러 모델을 독립적으로 학습시키고 이들의 예측을 평균내거나 다수결로 결합합니다. 예: 랜덤 포레스트(Random Forest).
    * 부스팅 (Boosting): 약한 모델을 순차적으로 학습시키며, 이전 모델이 잘못 예측한 부분에 가중치를 두어 점점 더 성능을 향상시킵니다. 예: 그라디언트 부스팅(Gradient Boosting), XGBoost.
    * 스태킹 (Stacking): 여러 다른 모델의 예측을 새로운 모델의 입력으로 사용하여 최종 예측을 만드는 기법입니다.

3. 의사결정 트리 (Decision Tree)

의사결정 트리는 데이터의 특성을 기반으로 의사결정을 트리 구조로 모델링한 것입니다. 트리의 각 노드는 특성에 대한 조건문을 나타내며, 가지(branch)는 조건에 따른 결과를 나타냅니다. 최종 리프 노드는 예측 결과를 나타냅니다. 주요 특징은 다음과 같습니다:

    *직관적 해석: 트리 구조로 인해 예측 과정이 매우 직관적이고 해석이 용이합니다.
    *비모수적 방법: 데이터의 분포에 대한 가정을 필요로 하지 않습니다.
    *카테고리 및 연속형 데이터 처리: 의사결정 트리는 다양한 유형의 데이터를 처리할 수 있습니다.

의사결정 트리는 단독으로 사용될 수도 있지만, 배깅(예: 랜덤 포레스트)이나 부스팅(예: AdaBoost, Gradient Boosting)과 같은 앙상블 기법에서 기본 모델로 자주 사용됩니다.

    6.1 k-최근접이웃 (k-Nearest Neighbors, KNN)

        KNN의 기본 개념

        KNN은 새로운 데이터 포인트를 예측할 때, 기존 데이터 포인트 중 가장 가까운 k개의 이웃을 찾고, 이 이웃들의 정보를 기반으로 예측을 수행합니다. "가까움"은 일반적으로 유클리드 거리와 같은 거리 측정 방법을 통해 정의됩니다.

        KNN 알고리즘의 작동 방식

            1. 훈련 데이터 준비: 알고리즘이 학습할 수 있도록 레이블이 지정된 훈련 데이터를 준비합니다.
            2. 새로운 데이터 포인트: 예측하려는 새로운 데이터 포인트를 제공합니다.
            3. 거리 계산: 새로운 데이터 포인트와 모든 훈련 데이터 포인트 간의 거리를 계산합니다.
            4. k개의 이웃 선택: 계산된 거리 중 가장 가까운 k개의 이웃을 선택합니다.
            예측:
                분류: 선택된 k개의 이웃 중 가장 많이 나타나는 클래스를 예측 값으로 지정합니다 (다수결 투표).
                회귀: 선택된 k개의 이웃의 평균 값을 예측 값으로 지정합니다.

        KNN의 특징

            * 단순함: 구현이 매우 간단합니다.
            * 비매개변수적: 사전에 모델을 학습시키지 않으며, 예측 시에만 계산이 이루어집니다.
            * 유연성: 분류와 회귀 모두에 사용할 수 있습니다.
            * 특성 스케일링: 거리 기반 알고리즘이기 때문에 특성 스케일링(정규화)이 중요합니다.

        KNN의 장단점

        장점:
            직관적이고 이해하기 쉬움: 알고리즘의 작동 방식이 명확하고 단순합니다.
            적응성: 데이터의 분포에 대한 가정을 필요로 하지 않습니다.
            다양한 문제에 적용 가능: 분류와 회귀 문제 모두에 사용할 수 있습니다.
        단점:
            계산 비용이 높음: 예측 시 모든 훈련 데이터 포인트와의 거리를 계산해야 하므로 데이터가 많을수록 계산 비용이 높아집니다.
            메모리 사용량이 큼: 모든 훈련 데이터를 저장하고 있어야 합니다.
            고차원 데이터에서 성능 저하: 고차원 데이터에서 거리 계산이 덜 효과적일 수 있습니다 (차원의 저주).

        KNN의 사용 사례
            이미지 인식: 비슷한 이미지들을 군집화하거나 분류하는 데 사용됩니다.
            추천 시스템: 사용자 선호도를 기반으로 비슷한 사용자들의 데이터를 참조하여 추천을 제공합니다.
            패턴 인식: 텍스트 분류, 음성 인식 등 다양한 패턴 인식 문제에 활용됩니다.

        KNN은 이해하기 쉽고 다양한 문제에 적용 가능한 유용한 알고리즘입니다. 다만, 대규모 데이터셋이나 고차원 데이터셋에서의 사용은 효율성을 고려해야 합니다.

        ** 용어정리 **
            *이웃(neighbor) : 예측변수에서 값들이 유사한 레코드
            *거리지표(distance metric) : 각 레코드사이가 얼마나 멀리 떨어져 있는지를 나타내는 단일값
            *표준화(standardization) : 평균을 뺀 후에 표준편차로 나누는 일(유의어 : 정규화)
            *z점수(z-score) : 표준화를 통해 얻은 값
            *K : 최근접 이웃을 계산하는데 사용되는 이웃의 개수

        6.1.1 예제 : 대출연체예측
        6.1.2 거리 지표
        6.1.3 원-핫 인코더
        6.1.4 표준화(정규화, z점수)
        6.1.5 선택하기
        6.1.6 KNN을 통한 피처 엔지니어링

    6.2 트리모델

    트리 모델(Tree Model)은 머신러닝에서 주로 사용되는 예측 모델의 한 유형으로, 트리 구조를 이용하여 의사 결정을 모델링합니다. 트리 모델의 대표적인 종류에는 의사결정 트리(Decision Tree), 랜덤 포레스트(Random Forest), 그리고 그라디언트 부스팅 트리(Gradient Boosting Tree) 등이 있습니다. 트리 모델의 주요 특징과 각각의 트리 모델을 간단히 설명하겠습니다.

    트리 모델의 주요 특징

    1. 트리 구조: 트리 모델은 루트 노드(root node)에서 시작하여 리프 노드(leaf node)로 끝나는 트리 구조를 가집니다. 각 내부 노드는 하나의 특성(feature)을 기반으로 데이터를 분할하고, 리프 노드는 최종 예측값을 나타냅니다.
    2. 규칙 기반 학습: 트리 모델은 데이터를 분할하기 위한 일련의 규칙을 학습합니다. 각 분할(split)은 데이터의 특성과 값을 기반으로 결정됩니다.
    3. 비모수적: 트리 모델은 데이터의 분포에 대한 가정을 필요로 하지 않습니다.
    4. 해석 가능성: 트리 모델은 의사 결정 과정을 시각적으로 표현할 수 있어, 모델의 작동 방식을 쉽게 이해하고 설명할 수 있습니다.

    ** 용어정리 **
        * 재귀분할(recursive partitioning) : 마지막 분할 영역에 해당하는 출력이 최대한 비슷한(homogeneous)결과를 보이도록 데이터를 반복적으로 분할하는 것
        * 분할값(split value) : 분하값을 기준으로 예측변수를 그 값보다 작은 영역과 큰 영역으로 나눈다.
        *마디(노드)node: 의사결정 트리와 같은 가지치기 형태로 구성된 규칙들을 집합에서, 노드는 분할규칙의 시각적인 표시라고 할 수 있다.
        *잎(leaf):if-then 규칙의 가장 마지막부분, 혹은 트리의 마지막 가지branch 부분을 의미한다. 트리모델에서 잎 노드는 어떤레코드에 적용할 최종적인 분류규칙을 의미한다.
        *손실(loss):분류하는과정에서 발생하는 오분류의 수, 손실이 클수록 불순도가 높다고 하 수 있다.
        *불순도(impurity):데이터를 분할한 집합엣 서로 다른 클래스의 데이터가 얼마나 섞여있는지를 나타낸다. 더 많이 섞여 있을수록 불순도가 높다고 할 수 있다.(유의어:이질성heterogenity, 반의어:동질성homogeneity, 순도)
        *가지치기(pruning):학습이 끝난 트리모델에서 오버피팅을 줄이기 위해 가지들을 하나씩 잘라내는 과정

        6.2.1 간단한예제
        6.2.2 재귀분할 알고리즘
        6.2.3 동질성과 불순도 측정하기
            -지니계수
            지니불순도를 지니계수(Gini coefficient)와 혼동해서는 안된다. 둘다 모두 개념적으로 비슷하지만 지니계수는 이진분류문제로 한정되며, AUC지표와 관련이 있는 용어이다.
        6.2.4 트리 형성 중지하기
        6.2.5 연속값 예측하기
        6.2.6 트리 활용하기
