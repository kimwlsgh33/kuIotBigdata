# 직접 활용해야지

어디에?

# 머신러닝 -> 딥러닝

## 머신러닝

빠르다

## 딥러닝

6.3 배깅과 랜덤 포레스트

- 앙상블(ensmeble) : 여러 모델의 집합을 이요해서 하나의 예측을 이끌어내는 방식(유의어:모델평균화[model averaging])
- 배깅(bagging) : 데이터를 부트스트래핑해서 여러 모델을 만드는 일반적인 방법(유의어:부트스트랩 종합[vootstrap aggregation])
- 랜덤 프레스트(random forest) : 의사 결정 트리 모델에 기반을 둔 배깅 추정모델(유의어:배깅 의사 결정 트리)
- 변수 중요도(variable importance) : 모델 성능에 미치는 예측변수의 중요도

  6.3.1 배깅 (Bagging)
  배깅(Bootstrap Aggregating)은 여러 약한 학습기를 결합하여 예측 성능을 향상시키는 앙상블 기법입니다. 배깅의 기본 아이디어는 원래 데이터셋에서 여러 개의 부트스트랩 샘플(중복을 허용한 랜덤 샘플)을 생성하고, 각 샘플에 대해 모델을 학습한 후 그 예측 결과를 평균내거나 다수결로 결정하는 것입니다. 배깅은 모델의 분산을 줄여 과적합을 방지하는 데 효과적입니다.

  6.3.2 랜덤포레스트 (Random Forest)
  랜덤포레스트는 배깅의 확장판으로, 여러 결정 트리를 앙상블하여 예측 성능을 향상시키는 방법입니다. 랜덤포레스트는 배깅의 기본 개념에 더해, 각 결정 트리가 학습할 때 데이터의 무작위 하위 집합뿐만 아니라 특성(변수)의 무작위 하위 집합도 선택합니다. 이는 트리들 간의 상관성을 줄여 모델의 예측 성능을 향상시키고, 과적합을 방지합니다. 랜덤포레스트는 높은 예측 정확도와 변수 중요도 측정의 장점이 있습니다.

  6.3.3 변수 중요도 (Feature Importance)
  랜덤포레스트와 같은 앙상블 모델은 변수 중요도를 평가할 수 있습니다. 변수 중요도는 각 특성이 모델의 예측에 얼마나 기여하는지를 나타냅니다.

  6.3.4 하이퍼파라미터
  하이퍼파라미터는 모델 학습 과정에서 사용자가 직접 설정해야 하는 매개변수로, 모델의 성능에 큰 영향을 미칩니다.

  6.4 부스팅

  **용어 정리**
  *앙상블(ensmeble) : 여러모델들의 집합을 통해 예측결과를 만들어 내는것 (유의어 : 모델 평균화)
  *부스팅(boosting) : 연속된 라운드마다 잔차가 큰 레코드들에 가중치 높여 일련의 모델들을 생성하는 일반기법
  *에이다부스트(AdaBoost) : 잔차에 따라 데이터의 가중치를 조절하는 부스팅의 초기버전
  *그레이디언트 부스팅(gradient boosting) : 비용함수(cost function)를 최소화하는 방향으로 부스팅을 활용하는 좀 더 일반적인 형태
  *확률적 그레이디언트 부스팅(stochastic gradient boosting) :각 라운드마다 레코드와 열을 재표본추출하는것을 포함하는 부스팅의 가장 일반적인 형태
  *정규화(regularization) : 비용함수에 모델의 파라미터 개수에 해당하는 벌점항을 추가해 오버피팅을 피하는 방법 \*하이퍼파라미터(hyperparameter) : 알고리즘을 피팅하기 전에 미리 세팅해야하는 파라미터

          6.4.1 부스팅 알고리즘
              부스팅(Boosting)은 여러 약한 학습기(weak learners)를 결합하여 강한 학습기(strong learner)를 만드는 앙상블 기법입니다. 약한 학습기란 예측 성능이 조금 낮은 모델을 의미하며, 이를 반복적으로 학습하여 예측 성능을 향상시키는 것입니다. 대표적인 부스팅 알고리즘에는 AdaBoost, Gradient Boosting, 그리고 XGBoost가 있습니다. 부스팅은 주로 순차적으로 모델을 학습시키며, 이전 모델이 틀린 데이터에 대해 가중치를 높여 다음 모델이 이 오류를 줄이도록 합니다.

          6.4.2 XG부스트
              XGBoost(eXtreme Gradient Boosting)는 Gradient Boosting 알고리즘의 확장판으로, 효율성과 성능이 매우 뛰어납니다. XGBoost는 다양한 최적화 기법과 정규화 기법을 도입하여 과적합을 방지하고, 빠른 학습과 예측을 가능하게 합니다. 또한, 분산 컴퓨팅을 지원하여 대규모 데이터셋에도 효과적으로 사용할 수 있습니다.

          6.4.3 정규화 : 오버피팅 피하기
              정규화는 모델의 복잡성을 줄여 과적합(overfitting)을 방지하는 기술입니다. 과적합은 모델이 학습 데이터에 너무 맞춰져서 새로운 데이터에 대해 일반화 성능이 떨어지는 현상입니다. 정규화 기법에는 L1 정규화(Lasso), L2 정규화(Ridge), 그리고 Elastic Net 등이 있습니다.

              * L1 정규화 (Lasso): 가중치의 절대값 합계를 최소화합니다. 이는 일부 가중치를 0으로 만들어 변수 선택(variable selection) 효과가 있습니다.
              * L2 정규화 (Ridge): 가중치의 제곱 합계를 최소화합니다. 이는 모든 가중치를 작게 만들어 모델의 복잡성을 줄입니다.
              * Elastic Net: L1과 L2 정규화를 결합한 방식으로, 두 가지 기법의 장점을 모두 취합니다.

          6.4.4 하이퍼파라미터와 교차타당성검사
              하이퍼파라미터는 모델 학습 과정에서 사용자가 직접 설정해야 하는 매개변수로, 모델의 성능에 큰 영향을 미칩니다. 예를 들어, 결정 트리의 깊이, 학습률(learning rate), 정규화 파라미터 등이 하이퍼파라미터에 해당합니다. 하이퍼파라미터는 학습 데이터에서 직접 학습되지 않으며, 사용자가 사전에 설정해야 합니다.

              교차타당성검사는 모델의 일반화 성능을 평가하기 위한 방법입니다. 데이터를 여러 개의 폴드(fold)로 나누고, 각 폴드에 대해 학습과 검증을 반복하여 모델의 성능을 평가합니다. 가장 일반적인 방법은 K-폴드 교차타당성(K-Fold Cross-Validation)입니다.

              K-폴드 교차타당성: 데이터를 K개의 폴드로 나누고, 각 폴드마다 한 번씩 검증 데이터를 사용하여 K번 학습을 반복합니다. 최종 모델 성능은 K번의 검증 결과를 평균하여 얻습니다.

Chap 7 비지도 학습 (Unsupervised Learning)

    비지도 학습은 데이터에 레이블(정답)이 없는 상황에서 패턴이나 구조를 찾는 머신러닝 기법입니다. 비지도 학습의 목표는 데이터의 숨겨진 구조를 발견하는 것입니다. 주요 비지도 학습 방법에는 클러스터링과 차원 축소가 있습니다.
    비지도학습이라는 용어는 레이블이 달린 데이터를 이용해 모델을 학습하는 과정없이 데이터로부터 의미를 이끌어내는 통계적기법들을 의미한다.

    클러스터링 (Clustering)

    클러스터링은 데이터를 유사한 특성을 가진 그룹(클러스터)으로 나누는 작업입니다. 클러스터링의 목적은 같은 클러스터 내의 데이터 포인트들이 서로 비슷하고, 다른 클러스터의 데이터 포인트들과는 다르게 만드는 것입니다. 대표적인 클러스터링 알고리즘은 다음과 같습니다:

    1. K-평균 클러스터링 (K-means Clustering):
        데이터를 K개의 클러스터로 나누는 알고리즘입니다.
        각 클러스터는 클러스터의 중심(centroid)을 기준으로 정의되며, 데이터 포인트는 가장 가까운 중심에 할당됩니다.
        반복적인 과정(군집의 중심 업데이트)을 통해 중심이 더 이상 변하지 않을 때까지 수행합니다.

    2. 계층적 클러스터링 (Hierarchical Clustering):
        데이터 포인트들을 계층적인 구조로 클러스터링합니다.
        상향식 방법(agglomerative): 각 데이터 포인트를 개별 클러스터로 시작하여, 가까운 클러스터끼리 합치는 방식으로 진행합니다.
        하향식 방법(divisive): 모든 데이터를 하나의 클러스터로 시작하여, 점진적으로 분할하는 방식으로 진행합니다.

    3. DBSCAN (Density-Based Spatial Clustering of Applications with Noise):
        밀도 기반 클러스터링 알고리즘입니다.
        데이터 포인트의 밀도를 기준으로 클러스터를 형성하며, 밀도가 높은 영역을 클러스터로 정의합니다.
        클러스터의 형태에 민감하지 않고, 노이즈와 이상치를 효과적으로 처리할 수 있습니다.

    차원 축소 (Dimensionality Reduction)

        차원 축소는 고차원 데이터를 저차원 공간으로 변환하는 과정입니다. 이는 데이터의 중요한 정보를 유지하면서 차원을 줄여 데이터 시각화, 노이즈 제거, 계산 비용 감소 등의 목적을 달성할 수 있습니다. 주요 차원 축소 방법은 다음과 같습니다:

    주성분 분석 (PCA, Principal Component Analysis):
        데이터의 분산을 최대한 보존하는 방향으로 주성분을 찾습니다.
        주성분은 데이터의 분산이 가장 큰 방향을 나타내며, 상호 직교합니다.
        데이터의 차원을 줄이면서 중요한 정보를 유지할 수 있습니다.

    선형판별분석 (LDA, Linear Discriminant Analysis):
        클래스 간 분산을 최대화하고, 클래스 내 분산을 최소화하는 방향으로 변환합니다.
        주로 분류 문제에서 사용되며, 레이블이 있는 데이터에 적용됩니다.

    t-SNE (t-Distributed Stochastic Neighbor Embedding):
        고차원 데이터를 저차원으로 변환하여 시각화하는 데 주로 사용됩니다.
        데이터 포인트 간의 거리를 보존하면서 저차원 공간으로 매핑합니다.
        비선형 구조를 잘 보존하며, 데이터의 클러스터 구조를 시각화하는 데 효과적입니다.

7.1 주성분 분석 (PCA, Principal Component Analysis)

        주성분 분석은 고차원 데이터를 저차원으로 축소하는 차원 축소 기법입니다. 데이터의 분산을 최대한 보존하는 방향으로 주성분을 찾아 데이터의 중요한 정보를 유지합니다. PCA는 데이터의 상관관계를 고려하여 주성분을 생성하며, 주성분들은 상호 직교(orthogonal)합니다.

        **용어정리**
            *주성분(principal component): 예측변수들의선형결합
            *부하(loading) : 예측변수들을 성분으로 변형할떄 사용되는 기중치(유의어:가중치)
            *스크리그래프(screeplot) : 성분들의 변동을 표시한 그림, 설명된 분산 혹은설명된 분산의 비율을 이용하여 성분들의 상대적인 중요도를 보여준다.
            *비지도 학습: 레이블이 없는 데이터에서 패턴이나 구조를 찾는 학습 방법입니다.
            *클러스터링: 데이터를 유사한 특성을 가진 그룹으로 나누는 방법으로, K-평균, 계층적 클러스터링, DBSCAN 등이 있습니다.
            *차원 축소: 고차원 데이터를 저차원으로 변환하는 방법으로, PCA, LDA, t-SNE 등이 있습니다.

    7.1.2 주성분 계산 (Principal Component Calculation)
        주성분을 계산하는 과정은 다음과 같습니다:
            1.데이터 표준화:
                데이터의 각 변수에 대해 평균을 0, 분산을 1로 맞추는 표준화를 수행합니다. 이는 변수의 단위 차이를 제거하고, 모든 변수의 중요도를 동일하게 고려하게 합니다.
                표준화된 데이터 행렬 XX를 얻습니다.

            2.공분산 행렬 계산:
                표준화된 데이터 행렬 XX의 공분산 행렬 ΣΣ를 계산합니다.
                공분산 행렬 ΣΣ는 각 변수 간의 공변동(covariance)을 나타내며, 이는 Σ=1n−1XTXΣ=n−11​XTX로 계산됩니다.

            3.고유값 및 고유벡터 계산:
                공분산 행렬 ΣΣ의 고유값(eigenvalue)과 고유벡터(eigenvector)를 계산합니다.
                고유값은 공분산 행렬의 분산을 설명하는 비율을 나타내며, 고유벡터는 주성분 방향을 나타냅니다.

            4.주성분 선택:
                고유값을 내림차순으로 정렬하여 가장 큰 고유값에 대응하는 고유벡터를 선택합니다.
                선택된 고유벡터가 주성분을 형성합니다.

            5.주성분 변환:
                원래 데이터를 선택된 주성분으로 변환하여 새로운 저차원 공간에 투영합니다.
                이 새로운 좌표가 주성분 점수(principal component scores)입니다.

        공변동[covariation]
        공변동은 두 변수 간의 상관관계를 나타냅니다. 두 변수 XX와 YY의 공분산 Cov(X,Y)Cov(X,Y)는 다음과 같이 계산됩니다:

        Cov(X,Y)=1n−1∑i=1n(Xi−Xˉ)(Yi−Yˉ)Cov(X,Y)=n−11​∑i=1n​(Xi​−Xˉ)(Yi​−Yˉ)

        여기서 XˉXˉ와 YˉYˉ는 각각 변수 XX와 YY의 평균입니다. 공분산이 양수이면 두 변수는 양의 상관관계를 가지며, 음수이면 음의 상관관계를 가집니다.

        7.1.3 주성분 해석 (Principal Component Interpretation)

        주성분 해석은 각 주성분이 원래 변수들의 어떤 조합으로 구성되었는지를 이해하는 과정입니다. 각 주성분은 원래 변수들의 선형 결합으로 표현되며, 주성분의 방향은 고유벡터에 의해 결정됩니다. 주성분의 크기(즉, 주성분 점수)는 고유값에 의해 결정됩니다.

        주성분의 해석은 다음과 같은 단계로 이루어집니다:
            1.고유벡터: 각 고유벡터의 요소는 주성분이 원래 변수에 대한 기여도를 나타냅니다.
            2.주성분 점수: 주성분 점수는 주성분으로 변환된 데이터를 의미하며, 각 데이터 포인트가 주성분 공간에서의 위치를 나타냅니다.
            3.주성분의 분산: 각 주성분이 데이터의 전체 분산에서 차지하는 비율을 이해합니다. 이는 주성분이 데이터의 변동성을 얼마나 잘 설명하는지를 나타냅니다.

        스크리 그래프 (Scree Plot)

        스크리 그래프는 고유값의 크기를 내림차순으로 표시하여, 데이터의 중요한 주성분 개수를 시각적으로 선택하는 데 도움을 줍니다. 스크리 그래프에서 고유값의 급격한 감소가 나타나는 지점을 찾는 것이 중요합니다. 이 급격한 감소 이후의 고유값들은 데이터의 변동성을 잘 설명하지 못하므로, 해당 지점 이전의 주성분들을 선택하는 것이 일반적입니다.

        스크리 그래프를 그리는 방법:
            1.고유값을 내림차순으로 정렬합니다.
            2.고유값을 y축에, 주성분 번호를 x축에 표시합니다.
            3.고유값의 급격한 감소가 나타나는 지점을 찾습니다. 이 지점 이후의 주성분은 무시할 수 있습니다.

        7.1.4 대응분석
        대응분석의 개요
        대응분석은 주성분 분석(PCA)과 유사한 수학적 원리를 사용하여, 범주형 데이터의 행과 열을 각각 주성분으로 변환합니다. 이를 통해 복잡한 다차원 데이터를 시각적으로 이해할 수 있는 2차원 또는 3차원 공간으로 축소합니다. 대응분석은 특히 마케팅, 사회과학, 생물학 등 다양한 분야에서 사용됩니다.

        대응분석의 주요 단계

        1.교차표 구성:
            분석할 범주형 변수들로 구성된 교차표를 작성합니다. 교차표의 각 셀은 두 범주형 변수의 빈도 또는 비율을 나타냅니다.
        2.행과 열의 프로파일 계산:
            각 행의 프로파일은 행 합계를 기준으로 표준화된 확률 분포를 나타냅니다.
            각 열의 프로파일은 열 합계를 기준으로 표준화된 확률 분포를 나타냅니다.
        3.카이제곱 거리 계산:
            카이제곱 거리는 두 범주형 변수 간의 차이를 측정하는 데 사용됩니다. 이 거리는 각 셀의 관찰된 빈도와 기대 빈도 간의 차이의 제곱을 기대 빈도로 나누어 계산합니다.
        4.고유값과 고유벡터 계산:
            카이제곱 거리 행렬의 고유값과 고유벡터를 계산하여, 각 범주의 주성분을 구합니다. 이는 PCA에서 공분산 행렬을 사용하는 것과 유사합니다.
        5.차원 축소 및 시각화:
            계산된 고유값과 고유벡터를 사용하여 데이터를 저차원 공간에 투영합니다. 일반적으로 2차원 또는 3차원 공간으로 축소하여 시각화합니다.

    대응분석의 해석
        *행과 열의 좌표: 대응분석의 결과로 얻어진 행과 열의 좌표는 저차원 공간에서 각 범주형 변수의 관계를 나타냅니다. 가까운 거리에 위치한 행과 열은 높은 연관성을 가지며, 멀리 떨어진 행과 열은 낮은 연관성을 가집니다.
        *주성분의 중요도: 각 주성분의 중요도는 고유값의 크기로 결정되며, 이는 데이터의 분산을 얼마나 설명하는지를 나타냅니다. 중요한 주성분은 데이터의 구조를 잘 설명합니다.
        *그래프 해석: 대응분석 결과를 시각화한 그래프에서는 각 점이 범주형 변수의 범주를 나타내며, 점들 간의 거리는 범주들 간의 관계를 나타냅니다. 이러한 시각화는 데이터의 구조적 패턴을 이해하는 데 도움을 줍니다.

    대응분석과 주성분 분석의 비교
        *데이터 유형: PCA는 연속형 데이터를, 대응분석은 범주형 데이터를 분석합니다.
        *수학적 원리: 두 기법 모두 고유값 분해(eigen decomposition)를 사용하지만, PCA는 공분산 행렬을, 대응분석은 카이제곱 거리 행렬을 사용합니다.
        *목표: PCA는 데이터의 분산을 최대한 보존하면서 차원을 축소하는 데 초점을 맞추고, 대응분석은 범주형 변수 간의 관계를 시각화하는 데 중점을 둡니다.

    대응분석 요약
        대응분석은 범주형 데이터의 구조적 관계를 분석하고 시각화하는 강력한 도구입니다. 주성분 분석과 유사한 수학적 원리를 사용하지만, 범주형 변수에 특화되어 있습니다. 대응분석을 통해 범주형 데이터의 숨겨진 패턴과 관계를 쉽게 이해할 수 있습니다.

7-2 k-평균 클러스터링
클러스터링은 데이터를 서로 다른 그룹으로 분류하는 기술을 말한다.
클러스터링의 목적은 데이터로부터 유의미한 그룹들을 구하는것이다.

        **용어정리**
            클러스터(cluster):서로 유사한 레코드들의 집합
            클러스터 평균(cluster mean) : 한 클러스터안에 속한 레코드들의 평균 벡터변수
            k : 클러스터의 개수

        7.2.1 간단한 예제
        7.2.2 k-평균 알고리즘
        7.2.3 클러스터 해석
